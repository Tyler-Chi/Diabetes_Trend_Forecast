{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c582d7f0-e00e-481d-8611-bdff6933959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_loss(history, save_dir=None, final_r2=None): \n",
    "    plt.title(\"Training loss curves\")\n",
    "    plt.plot(history.history['loss'], label='Train Loss') \n",
    "    plt.plot(history.history['val_loss'], label='Val Loss') \n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend() \n",
    "    plt.xticks(np.arange(0,len(history.history['loss']), 1.0), rotation=90)\n",
    "    \n",
    "    # if the final_r2 tuple is specified, then add descriiption for that at the bottom\n",
    "    if final_r2:\n",
    "        final_r2_description = f\"Train R2: {final_r2[0]:.5f}        Val R2: {final_r2[1]:.5f}\"\n",
    "        plt.figtext(0.5, -0.08, final_r2_description, horizontalalignment='center')\n",
    "    \n",
    "    plt.grid(True)\n",
    "    \n",
    "    # potentially save to file\n",
    "    if save_dir:\n",
    "        plt.savefig(f\"./{save_dir}/plot\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "# takes in a model, as well as the training data\n",
    "def evaluate_regression_model(\n",
    "    model,\n",
    "    history,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test\n",
    "):  \n",
    "    # first make the predictions\n",
    "    # pass the predictions into the r2 calculation\n",
    "    if history != None:\n",
    "        plot_loss(history)\n",
    "\n",
    "    train_predictions = model.predict(x_train)\n",
    "    test_predictions = model.predict(x_test)\n",
    "    \n",
    "    # flatten if necessary\n",
    "    if isinstance(train_predictions[0], list):\n",
    "        train_predictions = [pred[0] for pred in train_predictions]\n",
    "        test_predictions = [pred[0] for pred in test_predictions]\n",
    "    \n",
    "    # calculate r2 values\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    \n",
    "    print(\"======================================\")\n",
    "    print(f\"Train R2 score is {train_r2:.5}\")\n",
    "    print(f\"Test R2 score is {test_r2:.5}\")\n",
    "    print(\"======================================\")\n",
    "    \n",
    "    # plot train predicted vs actuals\n",
    "    sns.scatterplot(x=y_train, y=train_predictions)\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.title(\"Predicted vs Actual Train values\")\n",
    "    # plt.plot([1,5], [1,5], color=\"black\") # perfect line\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    # plot test predicted vs actuals\n",
    "    sns.scatterplot(x=y_test, y=test_predictions)\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.title(\"Predicted vs Actual Test values\")\n",
    "    # plt.plot([1,5], [1,5], color=\"black\") # perfect line\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e662f4-d082-4a3c-9cfa-ca7d019e848b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
