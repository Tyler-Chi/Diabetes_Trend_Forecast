{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c582d7f0-e00e-481d-8611-bdff6933959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_loss(history, save_dir=None, final_r2=None): \n",
    "    plt.title(\"Training loss curves\")\n",
    "    plt.plot(history.history['loss'], label='Train Loss') \n",
    "    plt.plot(history.history['val_loss'], label='Val Loss') \n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend() \n",
    "    plt.xticks(np.arange(0,len(history.history['loss']), 1.0), rotation=90)\n",
    "    \n",
    "    # if the final_r2 tuple is specified, then add descriiption for that at the bottom\n",
    "    if final_r2:\n",
    "        final_r2_description = f\"Train R2: {final_r2[0]:.5f}        Val R2: {final_r2[1]:.5f}\"\n",
    "        plt.figtext(0.5, -0.08, final_r2_description, horizontalalignment='center')\n",
    "    \n",
    "    plt.grid(True)\n",
    "    \n",
    "    # potentially save to file\n",
    "    if save_dir:\n",
    "        plt.savefig(f\"./{save_dir}/plot\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "# takes in a model, as well as the training data\n",
    "def evaluate_regression_model(\n",
    "    model,\n",
    "    history,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test\n",
    "):  \n",
    "    # first make the predictions\n",
    "    # pass the predictions into the r2 calculation\n",
    "    if history != None:\n",
    "        plot_loss(history)\n",
    "\n",
    "    train_predictions = model.predict(x_train)\n",
    "    test_predictions = model.predict(x_test)\n",
    "    \n",
    "    # flatten if necessary\n",
    "    if isinstance(train_predictions[0], list):\n",
    "        train_predictions = [pred[0] for pred in train_predictions]\n",
    "        test_predictions = [pred[0] for pred in test_predictions]\n",
    "    \n",
    "    # calculate r2 values\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    \n",
    "    print(\"======================================\")\n",
    "    print(f\"Train R2 score is {train_r2:.5}\")\n",
    "    print(f\"Test R2 score is {test_r2:.5}\")\n",
    "    print(\"======================================\")\n",
    "    \n",
    "    # plot train predicted vs actuals\n",
    "    sns.scatterplot(x=y_train, y=train_predictions, s=15)\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.title(\"Predicted vs Actual Train Values \")\n",
    "    plt.xlim([-150,150])\n",
    "    plt.ylim([-150,150])\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    # plot test predicted vs actuals\n",
    "    # plt.plot([-100,-100], [100,100], color=\"black\") # perfect line\n",
    "    plt.plot([-90,90], [-90,90], color=\"orange\") # perfect line\n",
    "    sns.scatterplot(x=y_test, y=test_predictions, s=15)\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.title(f\"Predicted vs Actual Test values (r2 = {test_r2:.4f})\")\n",
    "    plt.xlim([-100,100])\n",
    "    plt.ylim([-100,100])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54e662f4-d082-4a3c-9cfa-ca7d019e848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_feature_importance(importance,names, model_type, limit = None):\n",
    "    feature_importance = np.array(importance)\n",
    "    feature_names = np.array(names)\n",
    "\n",
    "    #Create a DataFrame using a Dictionary\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "    #Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "    \n",
    "    if limit:\n",
    "        fi_df = fi_df.head(limit)\n",
    "\n",
    "    #Define size of bar plot\n",
    "    # plt.figure(figsize=(10,8))\n",
    "    #Plot Searborn bar chart\n",
    "    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'], palette='Accent')\n",
    "    #Add chart labels\n",
    "    plt.title(model_type + ' Feature Importance')\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2b1f47-2b7f-438d-bf13-a7e2495fdcd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
